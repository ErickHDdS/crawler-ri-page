# LER_Bot

Este bot foi desenvolvido a partir do [repositório fornecido](https://github.com/daniel-hasan/ri-crawler) pelo professor [Daniel Hasan](https://github.com/daniel-hasan) da disciplina de Recuperação da Informação.

**Período:** maio e junho de 2025

## Integrantes do Grupo

- **Erick Henrique (erickhenriquedds@gmail.com)**
- **Lucas Cota (lucascdornelas@gmail.com)**
- **Rosane Silva (rosanesfaraujo@gmail.com)**

## Como funciona?

Um crawler é capaz de realizar buscas em páginas web, retornando os links encontrados na página e quaisquer outros subsequentes referenciados no conteúdo HTML.

O coletor foi desenvolvido com o intuito de estudar o funcionamento de um crawler ao realizar buscas em páginas web.

**Observação importante:** A coleta ocorre apenas nas páginas públicas de sites, sempre levando em consideração a política de exclusão de robôs (robots.txt).

## Funcionalidades Implementadas

### Coleta de Dados

- Extração automática de links de páginas web
- Processamento de conteúdo HTML
- Respeito às políticas robots.txt
- Navegação através de links subsequentes

### Características Técnicas

- Implementação baseada no framework fornecido
- Processamento eficiente de múltiplas páginas
- Tratamento de erros e exceções
- Logs detalhados de operação

## Resultados Obtidos

Durante o período de desenvolvimento, o crawler foi testado em diversos domínios, demonstrando eficácia na coleta e organização de dados web de forma automatizada.

### Métricas de Performance

- Páginas processadas com sucesso
- Tempo médio de processamento
- Links únicos coletados
- Conformidade com robots.txt

## Tecnologias Utilizadas

- **Python** - Linguagem principal
- **Bibliotecas de web scraping** - Para processamento HTML
- **Gestão de requisições HTTP** - Para comunicação com servidores
- **Processamento de texto** - Para extração de conteúdo

## Aprendizados

O desenvolvimento deste projeto proporcionou uma compreensão aprofundada sobre:

- Funcionamento de crawlers web
- Políticas de acesso a conteúdo online
- Processamento eficiente de grandes volumes de dados
- Boas práticas em coleta automatizada de informações

## Considerações Finais

O Crawler-Bot representa uma implementação educacional robusta de um sistema de crawler, respeitando as normas éticas de coleta de dados web e demonstrando os conceitos fundamentais da disciplina de Recuperação da Informação.

---

_Projeto desenvolvido como parte da disciplina de Recuperação da Informação - Junho 2025_
